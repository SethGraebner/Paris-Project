{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Main experimentation notebook. Code modularized to select a dataloader and vectorizer, and then to test various models with cross validation. Additional results / statistics generated once a classifier is selected on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoaders import AbstractDataLoader, OriginalDataLoader, StemDataLoader, NoSWLoader, GPTCleanedLoader, MatchLoader\n",
    "from Vectorizers import AbstractVectorizer, BOWVectorizer, W2VVectorizer, PreW2V, FastTextVectorizer, AdaVectorizer\n",
    "from type_utils import ProcessedData, MatchedData, UnprocessedData\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and vectorize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d63cabe5b80fe79fd3fb6b1072c7642b\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "dataloaders: List[AbstractDataLoader] = [\n",
    "    OriginalDataLoader(data_path='../william_data/test_xml/'),\n",
    "    StemDataLoader(data_path='../william_data/test_xml/'),\n",
    "    NoSWLoader(data_path='../william_data/test_xml/'), # loader without stopwords\n",
    "    GPTCleanedLoader(data_path='../william_data/test_xml/', cleaned_path='../william_data/cleanedjson/'),\n",
    "    MatchLoader(data_path='../william_data/test_xml/')\n",
    "]\n",
    "\n",
    "dataloader = dataloaders[3] # GPT\n",
    "# data: ProcessedData = dataloader.load_and_preprocess_data() # all but GPT loader\n",
    "data: UnprocessedData = dataloader.load_and_preprocess_data() # type: ignore # GPT loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "2290\n"
     ]
    }
   ],
   "source": [
    "print(len(data['good']))\n",
    "print(len(data['bad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from memory...\n",
      "(2618, 1536)\n",
      "(2618,)\n"
     ]
    }
   ],
   "source": [
    "vectorizers: List[AbstractVectorizer] = [\n",
    "    # BOWVectorizer(),\n",
    "    # W2VVectorizer(),\n",
    "    # PreW2V('frWac_non_lem_no_postag_no_phrase_200_skip_cut100'),\n",
    "    # PreW2V('fr_w2v_web_w5', always_reload=True, strategy=0), # private vectorizer, fewest words out of cache atm\n",
    "    # FastTextVectorizer(\"cc.fr.300\", always_reload=True, strategy=0),\n",
    "    AdaVectorizer()\n",
    "]\n",
    "\n",
    "vectorizer = vectorizers[0] # ada\n",
    "X, y = vectorizer.vectorize(data) # TODO: make sure you're using the same data the whole time, this doesn't check if the underlying data input is different when loading from the local cache. This includes the different preprocessing methods.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, stratify=y, random_state=17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr_sc = scaler.fit_transform(X_tr)\n",
    "X_te_sc = scaler.transform(X_te)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from CustomModels import AlwaysBad\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = {\n",
    "    'Bad': AlwaysBad(),\n",
    "    'LR': LogisticRegression(),\n",
    "    'SVM': SVC(),\n",
    "    'SGD-SVM': SGDClassifier(),\n",
    "    'NB': GaussianNB(),\n",
    "    'xgboost': XGBClassifier(colsample_bytree=0.6, subsample=0.7, n_estimators=100, max_depth=3, learning_rate=0.1),\n",
    "    'xgboost-best': XGBClassifier(colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=300, subsample=0.8),\n",
    "    'xgboost-nooverfit': XGBClassifier(colsample_bytree=0.4, learning_rate=0.2, max_depth=2, n_estimators=50, subsample=0.5, gamma=0.7, min_child_weight=1, eta=0.05, reg_alpha=0.1, reg_lambda=0.1),\n",
    "}   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# suppress sklearn warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate each model, with and without scaling, reporting accuracy, precision, recall, and f1\n",
    "best_model = None\n",
    "best_score = 0\n",
    "best_acc_model = None\n",
    "best_acc_score = 0\n",
    "for model_name, model in models.items():\n",
    "    print(f'Cross validating {model_name}...')\n",
    "    cv_results = cross_validate(model, X_tr, y_tr, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1']) # automatically stratified\n",
    "    print(f'Accuracy: {np.mean(cv_results[\"test_accuracy\"])}')\n",
    "    print(f'Precision: {np.mean(cv_results[\"test_precision\"])}')\n",
    "    print(f'Recall: {np.mean(cv_results[\"test_recall\"])}')\n",
    "    print(f'F1: {np.mean(cv_results[\"test_f1\"])}')\n",
    "    print()\n",
    "\n",
    "    if np.mean(cv_results[\"test_f1\"]) > best_score:\n",
    "        best_model = model_name\n",
    "        best_score = np.mean(cv_results[\"test_f1\"])\n",
    "\n",
    "    if np.mean(cv_results[\"test_accuracy\"]) > best_acc_score:\n",
    "        best_acc_model = model_name\n",
    "        best_acc_score = np.mean(cv_results[\"test_accuracy\"])\n",
    "\n",
    "    if model_name in ['SVM', 'SGD-SVM', 'LR']:\n",
    "        model_name += ' (scaled)'\n",
    "        print(f'Cross validating {model_name}...')\n",
    "        cv_results = cross_validate(model, X_tr_sc, y_tr, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "        print(f'Accuracy: {np.mean(cv_results[\"test_accuracy\"])}')\n",
    "        print(f'Precision: {np.mean(cv_results[\"test_precision\"])}')\n",
    "        print(f'Recall: {np.mean(cv_results[\"test_recall\"])}')\n",
    "        print(f'F1: {np.mean(cv_results[\"test_f1\"])}')\n",
    "        print()\n",
    "\n",
    "        if np.mean(cv_results[\"test_f1\"]) > best_score:\n",
    "            best_model = model_name\n",
    "            best_score = np.mean(cv_results[\"test_f1\"])\n",
    "\n",
    "        if np.mean(cv_results[\"test_accuracy\"]) > best_acc_score:\n",
    "            best_acc_model = model_name\n",
    "            best_acc_score = np.mean(cv_results[\"test_accuracy\"])\n",
    "\n",
    "\n",
    "print(f'Best model F1: {best_model}')\n",
    "print(f'Best score F1: {best_score}')\n",
    "print(f'Best model accuracy: {best_acc_model}')\n",
    "print(f'Best score accuracy: {best_acc_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: \\\n",
    "as of may 19, original strategies on preprocessing,\\\n",
    "xgboost best on public vecs does 0.953 F1, 0.082 test error, ~50/50 true good -> 95.4% F1 \\\n",
    "xgboost best on priv vecs does 0.952, 0.084 on te, 45/55 -> .953 F1 \\\n",
    "xgboost on our trained mini w2v does 0.937, .12, 20/80"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval on best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST = 'xgboost-best'\n",
    "clf = models[BEST]\n",
    "clf.fit(X_tr, y_tr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find train and test error\n",
    "print('Train error:', 1 - clf.score(X_tr, y_tr))\n",
    "print('Test error:', 1- clf.score(X_te, y_te))\n",
    "\n",
    "# find precision, recall, and f1\n",
    "y_pred = clf.predict(X_te)\n",
    "print('Precision:', precision_score(y_te, y_pred))\n",
    "print('Recall:', recall_score(y_te, y_pred))\n",
    "print('F1:', f1_score(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = clf.predict(X_te)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_te, y_pred)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import provide_confusion_matrix\n",
    "\n",
    "provide_confusion_matrix(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_te, y_pred)\n",
    "# f1\n",
    "print('F1:', f1_score(y_te, y_pred))\n",
    "\n",
    "provide_confusion_matrix(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provide_confusion_matrix(y_tr, clf.predict(X_tr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "priv vectors, no stopwords:\\\n",
    "avg: 51/48; 2/98\\\n",
    "min: 42; 1.3\\\n",
    "max: 38; 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

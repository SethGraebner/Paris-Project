{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\\\n",
    "Experimentation to check \"cache misses\" for vectorizer vocabularies. In essence, rudimentary methods result in only encoding ~half of our words - however many are due to OCR errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from DataLoaders import *\n",
    "from Vectorizers import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "\n",
    "dataloader: AbstractDataLoader = OriginalDataLoader(data_path='../william_data/test_xml/')\n",
    "data: ProcessedData = dataloader.load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer: AbstractVectorizer = PreW2V(path_to_bin='fr_w2v_web_w5')\n",
    "model: KeyedVectors = vectorizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every single word in the data, and if the word was not already checked, check if it is in the key_to_index. \n",
    "# keep a running tally of the number of words that are in the key_to_index, as well as which words are not in the key_to_index\n",
    "# after this experiment, create a simple plot using matplotlib to show the number of words that are in the key_to_index vs. the number of words that are not in the key_to_index\n",
    "\n",
    "in_vocab = []\n",
    "out_vocab = []\n",
    "\n",
    "for excerpt in data['good'] + data['bad']:\n",
    "    for word in excerpt:\n",
    "        if word not in in_vocab and word not in out_vocab:\n",
    "            if word in model.key_to_index:\n",
    "                in_vocab.append(word)\n",
    "            else:\n",
    "                out_vocab.append(word)\n",
    "\n",
    "print(f'Number of words in vocab: {len(in_vocab)}')\n",
    "print(f'Number of words not in vocab: {len(out_vocab)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "public: \\\n",
    "Number of words in vocab: 30078 \\\n",
    "Number of words not in vocab: 20640 \\\n",
    "Saved as \"in_vocab_public.pkl\"\n",
    "\n",
    "private: \\\n",
    "Number of words in vocab: 34505\\\n",
    "Number of words not in vocab: 16213\\\n",
    "saved as similar pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out_vocab) / (len(in_vocab) + len(out_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in out vocab from pkl\n",
    "out_vocab_public = pickle.load(open('out_vocab_public.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of words that are in out_vocab_public but not in out_vocab\n",
    "out_vocab_public_not_in_vocab = []\n",
    "for word in out_vocab_public:\n",
    "    if word not in out_vocab:\n",
    "        out_vocab_public_not_in_vocab.append(word)\n",
    "\n",
    "print(f'Number of words in out_vocab_public but not in out_vocab: {len(out_vocab_public_not_in_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab_public_not_in_vocab[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sample 100 words from the out_vocab_idxs list\n",
    "sampled_out_vocab = np.random.choice(out_vocab, 1000)\n",
    "\n",
    "sampled_out_vocab\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like almost entirely OCR errors...\n",
    "\n",
    "Moving on to checking stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('french') + ['ici', 'là', 'elles', 'trop', 'tous', 'selon', 'presque', 'tant', \n",
    "                                                'fois', 'quant', 'ainsi', 'cette', 'doit', 'tout', 'bien', 'toute', \n",
    "                                                'si', 'autre', 'sans', 'comment', 'rien', 'là', 'peu', 'mêmes', 'si', \n",
    "                                                'plutôt', 'ceux', 'faire', 'moins', 'être', 'faudra', \n",
    "                                                'deux', 'a', 'paris', 'plus', 'où', 'saint', 'cette'])\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the stopwords are in the key_to_index\n",
    "sw_in_vocab = []\n",
    "sw_out_vocab = []\n",
    "\n",
    "for word in sw:\n",
    "    if word not in sw_in_vocab and word not in sw_out_vocab:\n",
    "        if word in model.key_to_index:\n",
    "            sw_in_vocab.append(word)\n",
    "        else:\n",
    "            sw_out_vocab.append(word)\n",
    "\n",
    "print(f'Number of stopwords in vocab: {len(sw_in_vocab)}')\n",
    "print(f'Number of stopwords not in vocab: {len(sw_out_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_out_vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from public, ['eûtes',\n",
    " 'fusses',\n",
    " 'ayantes',\n",
    " 'étante',\n",
    " 'ayante',\n",
    " 'étantes',\n",
    " 'étées',\n",
    " 'fussiez',\n",
    " 'eusses',\n",
    " 'fussions'] \\\n",
    "\n",
    " from private ['étantes', 'étées', 'ayante', 'étante', 'ayantes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "ft = fasttext.load_model('cc.fr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using, ft repeat the same process above of finding % of words in and out of vocab\n",
    "from tqdm import tqdm\n",
    "in_vocab_ft = []\n",
    "out_vocab_ft = []\n",
    "\n",
    "words = ft.words\n",
    "\n",
    "for excerpt in tqdm(data['good'] + data['bad']):\n",
    "    for word in excerpt:\n",
    "        if word in words:\n",
    "            in_vocab_ft.append(word)\n",
    "        else:\n",
    "            out_vocab_ft.append(word)\n",
    "\n",
    "in_vocab_ft = set(in_vocab_ft)\n",
    "out_vocab_ft = set(out_vocab_ft)\n",
    "\n",
    "print(f'Number of words in vocab: {len(in_vocab_ft)}')\n",
    "print(f'Number of words not in vocab: {len(out_vocab_ft)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning experiments / math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoaders import MatchLoader\n",
    "ml = MatchLoader(data_path='../william_data/test_xml/')\n",
    "matches = ml.load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of characters in all the matches\n",
    "total = 0\n",
    "for match in matches['bad'] + matches['good']:\n",
    "    total += len(match['snippet'])\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = (total * 2) / 3\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = (tokens / 1000) * .002\n",
    "price"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demo of vector vocabulary misses on old vs new ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../william_data/ocr_output/res.txt') as f:\n",
    "    new_ocr = f.read()\n",
    "with open('../william_data/ocr_output/Balzac_1841_bpt6k1133819_CL.txt') as f:\n",
    "    old_ocr = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_text(text: str) -> List[str]:\n",
    "    clean_text = re.sub(r'[^\\s0123456789abcdefghijklmnopqrstuvwxyzàâäæçèéêëîïñôùûüÿœ̀œ]',\n",
    "                    ' ',\n",
    "                    text.lower())\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "\n",
    "    return clean_text.split()\n",
    "\n",
    "new_ocr_tokens = _tokenize_text(new_ocr)\n",
    "old_ocr_tokens = _tokenize_text(old_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ocr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vectorizers import *\n",
    "vectorizer: AbstractVectorizer = PreW2V(path_to_bin='fr_w2v_web_w5')\n",
    "model: KeyedVectors = vectorizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every single word in the data, and if the word was not already checked, check if it is in the key_to_index. \n",
    "# keep a running tally of the number of words that are in the key_to_index, as well as which words are not in the key_to_index\n",
    "# after this experiment, create a simple plot using matplotlib to show the number of words that are in the key_to_index vs. the number of words that are not in the key_to_index\n",
    "\n",
    "in_vocab = []\n",
    "out_vocab = []\n",
    "\n",
    "\n",
    "for word in new_ocr_tokens:\n",
    "    if word not in in_vocab and word not in out_vocab:\n",
    "        if word in model.key_to_index:\n",
    "            in_vocab.append(word)\n",
    "        else:\n",
    "            out_vocab.append(word)\n",
    "\n",
    "print(f'Number of words in vocab: {len(in_vocab)}')\n",
    "print(f'Number of words not in vocab: {len(out_vocab)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new: 3876, 171\n",
    "old: 3902, 403"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

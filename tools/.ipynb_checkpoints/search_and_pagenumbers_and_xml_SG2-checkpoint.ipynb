{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm testing with a corpus . . . \n",
    "\n",
    " . . . with one file.  I test all of this less the line number logic using another, larger corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files in path\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "paths_to_files = glob.glob('/home/seth/Notes/HDW/test/*.txt')\n",
    "\n",
    "print(len(paths_to_files),'files in path')\n",
    "\n",
    "\n",
    "terms_to_find= [r'Notre-\\s*Dame', 'Cité', 'Saint-\\s*Louis', 'Arènes', \n",
    "r'Palais\\s*de\\s*Justice|Palais-\\s*de-\\s*Justice',\n",
    "'Morgue', r'Sainte-\\s*Chapelle', 'Conciergerie', r'[Qq]uai\\s*de\\s*l\\'Horloge', r'Pont-\\s*Neuf', r'Cluny|Thermes',\n",
    "r'Saint-\\s*Germain-\\s*des-\\s*Prés', 'Nesle', r'[Ss]aint-\\s*Sulpice', r'[Pp]alais\\s*du\\s* Luxembourg', \n",
    "r'[Jj]ardin\\s*du\\s*Luxembourg', 'Observatoire', r'Panthéon|Sainte-\\s*Geneviève', r'[Eéeé]glise\\s*Saint-\\s*Étienne',\n",
    "'Odéon', r'[Jj]ardin\\s*des\\s*Plantes', 'Gobelins', 'Auxerrois', 'Louvre', r'Carrousel|Doyenné', 'Tuileries', \n",
    "r'Palais-\\s*Royal', r'Comédie-\\s*Française', 'Bourse', 'Innocents', 'Halles', r'Saint-\\s*Eustache', 'Temple',\n",
    "r'[Tt]our\\s*Saint-\\s*Jacques', r'H[oôóòö]tel\\s*de\\s*Ville|Gr[eêéèë]ve', 'Rivoli', \n",
    "r'Bastille|[Cc]olonne\\s*de\\s*Juillet', 'Tournelles', r'[Bb]oulevar[dt]\\s*de\\s*la\\s*Madeleine', 'Capucines',\n",
    "r'[Bb]oulevard\\s*des\\s*Italiens', r'[Bb]oulevar[dt]\\s*Montmartre', r'[Bb]oulevar[dt]\\s*Poissonnière',\n",
    "r'[Bb]oulevar[dt]\\s*Bonne-\\s*Nouvelle', r'[Bb]oulevar[dt]\\s*Saint-\\s*Denis', r'[Bb]oulevar[dt]\\s*Saint-\\s*Martin',\n",
    "r'[Bb]oulevar[dt]\\s*du\\s*Temple|[Bb]oulevard\\s*du\\s*crime', r'[Bb]oulevar[dt]\\s*des\\s*Filles', 'Beaumarchais',\n",
    "r'[Pp]orte\\s*Saint-Denis', r'[Cc]afé\\s*Tortoni', r'[Cc]afé\\s*Anglais', 'Maison-\\s*Dorée', \n",
    "r'Notre-\\s*Dame-\\s*de-\\s*Lorette', 'Opéra-\\s*[Cc]omique', 'Panorama', 'Opéra', r'[Aa]venue\\s*\\s*de\\s*l’Op[eé]ra',\n",
    "r'[Rr]ue\\s*de\\s*la\\s*Paix|[Rr]ue\\s*\\de\\s* Napoléon', 'Vivienne', r'[Rr]ue\\s*Saint-\\s*Jacques', \n",
    "r'[Rr]ue\\s*Saint-\\s*Denis', r'[Ff]aubourg\\s*Saint-\\s*Honoré', r'[Rr]ue\\s*du\\s*[Ff]aubourg\\s*Saint-\\s*Antoine',\n",
    "r'[Ff]aubourg\\s*Saint-\\s*Antoine', r'[Pp]lace\\s*des\\s*Vosges|Place\\s*Royale', r'Champs-\\s*Elysées', \n",
    "r'Concorde|[Pp]lace\\s*Louis[.\\s*]XV|obélisque', r'[EÉ]toile|Triomphe', 'Vend[oôóòö]me', r'[Ll]a\\s*Madeleine', \n",
    "'Caire', r'des\\s*Miracles', r'Quinze-\\s*Vingts', r'cimeti[eêéèë]re\\s*du\\s*P[eêéèë]re-\\s*Lachaise,' \n",
    "r'[Bb]utte\\s*Montmartre', r'Montfaucon|[Vv]oierie', 'Chaumont', r'[Cc]h[aâáàä]teau\\s*de\\s*Vincennes', 'Invalides',\n",
    "r'[Eéeé]cole\\s*Militaire|Champ-\\s*de-\\s*Mars', 'Grenelle']                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A routine for writing XML\n",
    "\n",
    "We call this from below . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def write_results_to_xml(file_name, results):\n",
    "\n",
    "    xml_root = etree.Element('matches_and_snippets')\n",
    "    \n",
    "    for r in results:\n",
    "        \n",
    "        match_node = etree.Element('match')\n",
    "        xml_root.append(match_node)\n",
    "        \n",
    "        uid_node = etree.Element('uid')\n",
    "        uid_node.text = ''\n",
    "        match_node.append(uid_node)\n",
    "        \n",
    "        file_name_node = etree.Element('file_name')\n",
    "        file_name_node.text = r['file_name']\n",
    "        match_node.append(file_name_node)\n",
    "    \n",
    "        place_name_node = etree.Element('place_name')\n",
    "        place_name_node.text = ''\n",
    "        match_node.append(place_name_node)\n",
    "        \n",
    "        pdf_page_number_node = etree.Element('pdf_page_number')\n",
    "        pdf_page_number_node.text = str(r['pdf_page_number'])\n",
    "        match_node.append(pdf_page_number_node)\n",
    "        match_node.append(file_name_node)\n",
    "        \n",
    "        source_author_node = etree.Element('source_author')\n",
    "        source_author_node.text = ''\n",
    "        match_node.append(source_author_node)\n",
    "        \n",
    "        source_title_node = etree.Element('source_title')\n",
    "        source_title_node.text = ''\n",
    "        match_node.append(source_title_node)\n",
    "        \n",
    "        source_date_node = etree.Element('source_date')\n",
    "        source_date_node.text = ''\n",
    "        match_node.append(source_date_node)\n",
    "        \n",
    "        source_page_number_node = etree.Element('source_page_number')\n",
    "        source_page_number_node.text = ''\n",
    "        match_node.append(source_page_number_node)\n",
    "        \n",
    "        matching_term_node = etree.Element('matching_term')\n",
    "        matching_term_node.text = r['matching_term']\n",
    "        match_node.append(matching_term_node)\n",
    "        \n",
    "        cleaner_node = etree.Element('cleaner_name')\n",
    "        cleaner_node.text = ''\n",
    "        match_node.append(cleaner_node)\n",
    "        \n",
    "        date_added_node =etree.Element('date_added')\n",
    "        date_added_node.text = ''\n",
    "        match_node.append(date_added_node)\n",
    "        \n",
    "        snippet_node = etree.Element('snippet')\n",
    "        snippet_node.text = r['snippet']\n",
    "        match_node.append(snippet_node)\n",
    "        \n",
    "    xml_root.getroottree().write(file_name.replace('.txt', '.xml'), encoding='utf-8', pretty_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using (very straight-forward) regular expressions\n",
    "\n",
    "The regular expression can be just a string, like \"Saint-Gervais\".\n",
    "\n",
    "Something like \n",
    "\n",
    "> r'Dépôt\\s*de\\s*mendicité\\s*de\\s*Saint-Denis' \n",
    "    \n",
    "lets us search across line breaks, even when the OCR has inserted extra line breaks.  Note that the leading r matters.\n",
    "\n",
    "> r'Tour\\s*Saint-Jacques'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "N_CHARACTERS_BEFORE_MATCH = 1000\n",
    "N_CHARACTERS_AFTER_MATCH = 2000\n",
    "\n",
    "for path_to_file in paths_to_files:\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Get the file name less the path; we use the full path to read, and this file name in the printed output\n",
    "    file_name = path_to_file.split('/')[-1]\n",
    "    \n",
    "    text = open(path_to_file).read()\n",
    "    \n",
    "    # Where are the form-feed characters in the document?\n",
    "    \n",
    "    page_break_locations = []\n",
    "    \n",
    "    for match in re.finditer('\\x0c', text):\n",
    "        page_break_locations.append(match.start())\n",
    "    \n",
    "    # The actual term matching\n",
    "    \n",
    "    for term in terms_to_find:\n",
    "        \n",
    "        snippet_ending_position = 0\n",
    "        \n",
    "        for match in re.finditer(term, text):\n",
    "                                  \n",
    "            pdf_page_number = 1\n",
    "            for location in page_break_locations:\n",
    "                if location > match.start():\n",
    "                    break\n",
    "                pdf_page_number += 1\n",
    "            \n",
    "            if match.start() < snippet_ending_position:\n",
    "                continue\n",
    "            \n",
    "            snippet_starting_position = match.start() - N_CHARACTERS_BEFORE_MATCH\n",
    "            if snippet_starting_position < 0:\n",
    "                snippet_starting_position = 0\n",
    "            \n",
    "                            \n",
    "            snippet_ending_position = match.end() + N_CHARACTERS_AFTER_MATCH\n",
    "            if snippet_ending_position > len(text):\n",
    "                snippet_ending_position = len(text)\n",
    "            \n",
    "                           \n",
    "            snippet = text[snippet_starting_position: snippet_ending_position]\n",
    "            \n",
    "            results.append({'file_name': file_name, \n",
    "                            'pdf_page_number': pdf_page_number,\n",
    "                            'matching_term': text[match.start(): match.end()], \n",
    "                            'snippet': snippet.replace('\\x0c', '')})\n",
    "            \n",
    "    if len(results) > 0:\n",
    "        \n",
    "        write_results_to_xml(file_name, results)\n",
    "#print(type(results))        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
